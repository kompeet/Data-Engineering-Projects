## ETL pipeline

Create a Python project which retrieves the data from [this API](https://restcountries.com/#api-endpoints-v3-list-of-codes) (find the endpoint that returns ALL countries) and load the data into MSSQL database (all fields). 

Store info to the MSSQL database under the table _countries_ with following columns:
- id (has to be autogenerated, integer)
- name ('common')
- iso2_code ('cca2')
- continent (1. element of the continents)

(In the brackets you find the exact fields that we are interested in.)
## Extract airports data, transform and load

Create an Airflow DAG which runs weekly, which extract data from airports dataset. Download the csv file from the [dataset](https://datahub.io/core/airport-codes) or use this url to [json file](https://pkgstore.datahub.io/core/airport-codes/airport-codes_json/data/9ca22195b4c64a562a0a8be8d133e700/airport-codes_json.json) (no need to download it) to extract the dataset. 

Your next job is to transform the dataset so we will be able to work with it:
- If gps_code of the airport is missing: drop the row
- if some fields are empty, fill it with zero.

_Hint: Make sure you have the same order of columns in the pandas Dataframe as in the MSSQL table._

After transforming, store the data into MSSQL table _airports_ with following columns:
- id (has to be autogenerated, integer)
- ident
- type
- name
- elevation_ft (integer)
- iso_country
- iso_region
- municipality
- gps_code
- iata_code
- local_code
- coordinates

These two dataset have something in common.
The relationship between the tables is: One country can have more airports, but one airport belongs only to one country!
You might need this information when joining the tables.

Amount of DAG tasks is up you. Because of inserting more then 40k rows, this task may take a while in Airflow.
(If you have performance issues during the load phase, you can limit the rows you load in the database.)

(Export the db from Azure Data Studio as csv. If you create the db locally, give us the query in a separate file.)

## Load data to MongoDB
Retrieve name, type, elevation_ft, iso_region, gps_code, local_code of airport and name, continent from countries and insert the result to MongoDB collection airports as separate documents.

## Create Reports

Your job is to create a separate project (Airflow DAG) which create some reports. 

Write the following SQL queries (you can just print the result):
- What are the top 10 highest airports based on the `elevation_ft` (with ties)?
- Which airports are located in Africa?
- Which airports local code start with 02? Return only name and local_code.
- What are the highest elevation_ft by continents?
- What is the count of airports that belongs to small_airport airport type?

Write the following Mongo queries (you can just print the result):
- Which airports are located in Europe? Return only name and continent and sort the result by elevation_ft in descending order.
- What is the count of airports that belongs to heliport airport type?

(Create a DAG and use pymongo. Export the db from MongoDB Compass as json.)
